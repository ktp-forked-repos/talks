## Text back end

Now the fun part begins, we are going to deploy our first python application
on OpenShift.

### Flask application

This time, let's try using the web console to create the text back end.
Click __Add to project__ at the top and look for Python in the __Browse Catalog__
tab. On the next page leave the version (`3.5 - latest`) as is, and click __Select__. On the final page fill in the necessary details: name, git
repository, and context directory (after clicking __show advanced options__ at
the bottom):

image::text_app.png[Test Application]

Alternatively, you can use the CLI. This time we do not specify the builder
image, that is because `oc new-app` should figure out that we are trying to
deploy a python application, and use appropriate builder image for us:

[source,bash]
----
oc new-app \
    https://github.com/soltysh/blast.git \
    --context-dir=text \
    --name=text \
    --image-stream=python:3.5 \
    --labels=app=text

oc expose service/text
----

As you have probably noticed, the advanced options revealed a whole bunch of
configuration options for build and deployment. Additionally, the web console
created the Route for us.

We can navigate to the __Overview__ page and check the progress of our build by
clicking __#1__ next to build name and then choosing __Logs__ tab.

image::build_logs.png[Build Logs]

Once the application builds, we should have the text back end up and running.

Oops, it looks like it is not working as expected, let's debug and check what
went wrong.

image::failed_deployment.png[Failed Deployment]

Click __text__ to get to the deployment configuration details,

image::deployment_1.png[Deployment #1]

then click __#1__ to get into our first deployment details,

image::deployment_pods.png[Deployment Pods]

and click the first Pod in the list,

image::failed_pod.png[Failed Pod]

and switch to __Logs__ tab to verify what went wrong:

image::pod_logs.png[Pod Logs]

Alternatively, you can use the following series of commands for debugging:

[source,bash]
----
oc get pod -l app=text
----

Which will return you a list of pods associated with our application. The
`-l app=text` filters the pods to all, that have the label `app` set to `text`.
This happens automatically for every application created using the web
console, but can be done manually when using `oc new-app` with `--labels` flag.
To get the logs of a desired pods run:

[source,bash]
----
oc logs pod/text-1-p07pm
----

[NOTE]
====
Make sure to use your own Pod name with `oc logs` command.
====

The logs clearly point that our deployment is missing `APP_MODULE` environment
variable. Let's check the link:https://docs.openshift.org/latest/using_images/s2i_images/python.html#configuration[python S2I builder image documentation]
to figure out what that is.

[NOTE]
====
This variable specifies the WSGI callable. It follows the pattern
`$(MODULE_NAME):$(VARIABLE_NAME)`, where the module name is a full dotted path
and the variable name refers to a function inside the specified module.
====

Looking into the link:https://github.com/soltysh/blast/blob/master/text/api.py[application source code]
we see that it should be set to `api:app`. Go back to deployment configuration
and click on the __Environment__ tab to set the value accordingly:

image::set_env.png[Set Environment Variable]

Alternatively:

[source,bash]
----
oc set env deploymentconfig/text APP_MODULE=api:app
----

After applying the change our application should be up and running.

Like previously, let's configure the application liveness and readiness probes,
to leverage platform's health checks and provide always running application:

[source,bash]
----
oc set probe deploymentconfig/text --liveness --open-tcp=8080
oc set probe deploymentconfig/text --readiness --get-url=http://:8080/blast/api/v1.0/text/x
----

Now, that we have added the probes, we see that the application is not being
deployed properly. Let's follow the same path as previously and try to debug
what is wrong with it. This time, though let's check the __Events__ tab
first:

image::failed_readiness.png[Failed Readiness]

It looks like our newly added readiness probe failed. Checking Pod logs shows it had problem connecting to MongoDB database:

image::text_logs.png[Text Logs]

### MongoDB database

Since we do not remember (or do not know) which database images are available
in the cluster, let's look around. Click __Add to project__ at the top and
pick __Deploy Image__ tab, this time. By default, each OpenShift installation
comes with a set of pre-defined Image Streams. These include different
language runtimes, as well as database images. They are all installed in the
`openshift` project.

In that case, let's pick the `openshift` namespace and see the list of the
available Image Streams. Pick `mongodb` from that list and tag `3.2`:

image::mongodb.png[MongoDB]

As usual, the CLI alternative is:

[source,bash]
----
oc get imagestream -n openshift
----

to verify which Image Streams are available in the `openshift` namespace and:

[source,bash]
----
oc new-app mongodb:3.2 \
    --name=text-db \
    --labels=app=text
----

to create MongoDB deployment.

Apparently, our mongodb deployment is not working properly. Once again let's
try to figure out the reason of that failure:

image::mongo_logs.png[MongoDB Logs]

OpenShift provided images have a set of required environment variables to be
working properly. They are all documented in link:https://docs.openshift.org/latest/using_images/db_images/index.html[database image documentation].
Checking link:https://docs.openshift.org/latest/using_images/db_images/mongodb.html#configuration-and-usage[MongoDB usage documentation]
and looking at the logs we clearly see we need to set those three values:

- `MONGODB_USER` - user name for the MongoDB account.
- `MONGODB_PASSWORD` - password for the user account.
- `MONGODB_DATABASE` - database name.
- `MONGODB_ADMIN_PASSWORD` - password for the database admin.

But, since we will need those values in several places inside our application
we would like to have a single place for that configuration. This is where a
ConfigMap comes in handy.

### ConfigMap

link:https://docs.openshift.org/latest/dev_guide/configmaps.html[ConfigMap]
provides mechanisms to inject configuration data while keeping containers
agnostic of OpenShift Origin. Configuration data can be consumed in Pods in a
variety of ways. A ConfigMap can be used to:

- populate the value of environment variables.
- set command-line arguments in a container.
- populate configuration files in a volume.

We are going to leverage the environment variables injection mechanism in our
application. For that, while on the `text-db` deployment configuration page,
see __Volumes__ section, there is a link that allows you to __Add Config Files__:

image::add_config.png[Add ConfigMap]

On the next screen, click __Create ConfigMap__, and fill it in with the
required data:

- `MONGODB_USER`
- `MONGODB_PASSWORD`
- `MONGODB_DATABASE`
- `MONGODB_ADMIN_PASSWORD`

image::configmap_value.png[ConfigMap Value]

CLI alternative is as follows:

[source,bash]
----
oc create configmap text \
    --from-literal=MONGODB_USER=13eewfsd23rwef23re \
    --from-literal=MONGODB_PASSWORD=wewsfsd234eewdsa \
    --from-literal=MONGODB_ADMIN_PASSWORD=45trre23qwedssdf \
    --from-literal=MONGODB_DATABASE=blast_text
----

[NOTE]
====
Both user name and password can be random strings, but database name needs to
be set to `blast_text`.
====

Like it was mentioned a ConfigMap can be consumed in a multiple ways. We are
going to inject its values using environment variables. To do so, use the
following command:

[source,bash]
----
oc set env deploymentconfig/text-db --from=configmap/text
oc set env deploymentconfig/text --from=configmap/text
----

We are feeding both our deployments with this newly created ConfigMap. This
should result in both of them up and running after a short while.

We can verify it the application is actually running by accessing following
address `https://text-<project>.{{ROUTER_ADDRESS}}/blast/api/v1.0/text/x`.
